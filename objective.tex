The main objectives of our work.
\begin{enumerate}
\item UCB-Improved has several shortfalls. It conducts too much early exploration and the arm elimination is very conservative. Our proposed algorithm tries to remedy this. We also explore the idea of employing clustering techniques in MAB which, till now has only been studied in the contextual bandit setup (see \cite{li2010contextual}, \cite{bui2012clustered}, \cite{cesa2013gang} , \cite{gentile2014online}, \cite{nguyen2014dynamic}). We propose an algorithm that minimizes cumulative regret in the multi-armed stochastic bandit setup using clustering and improved exploration and performs at par with the best algorithms currently available. We also show that there is a distinct advantage of using clustering in action elimination type of algorithms in MABs.
\item We also propose an algorithm that minimizes probability of error in a combinatorial stochastic bandit setup whereby the learning algorithm, provided with a threshold $\tau$, proposes the best set of arms such that $r_{i}\geq \tau$ at every timestep using action elimination method. This problem is similar to the pure exploration setup with a major difference being that the algorithm has to suggest as many arms whose $r_{i}$ is above $\tau$ which may be more than one arm. Our proposed methodology is in contrast with APT which uses an UCB1 type of strategy to suggest the best arms above the threshold whereas we employ an UCB-Improved type of strategy with better exploration parameters.
\item In both the above scenarios we intend to explore both the theoretical guarantees and empirical performance and verify against the best algorithms available in the field.     
\end{enumerate}