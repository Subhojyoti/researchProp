The main objectives of our work.
\begin{enumerate}
\item We propose an algorithm that minimizes cumulative regret in the multi-armed stochastic bandit setup  and performs at par with the best algorithms currently available.
\item We also propose an algorithm that minimizes probability of error in a combinatorial stochastic bandit setup whereby the learning algorithm, provided with a threshold $\tau$, proposes the best set of arms such that $r_{i}\geq \tau$ at every timestep. This problem is similar to the pure exploration setup with a major difference being that the algorithm has to suggest as many arms whose $r_{i}$ is above $\tau$ which may be more than one arm.
\item In both the above scenarios we intend to explore both the theoretical guarantees and empirical performance and verify against the best algorithms available in the field.     
\end{enumerate}