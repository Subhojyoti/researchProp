%In this paper, we propose a novel Augmented-UCB framework where we use the fast exploring nature of UCB1 accompanied by a novel modification of UCB-Revisited, which includes two arm deletion and early stopping conditions. We propose a method where we use UCB1(non-round based) to conduct an initial exploration of the arms and then transition to a variation of UCB Revisited(round based) which has a better performance than the original one.
%In the stochastic bandit scenario UCB-Revisited performs very badly when the $\Delta_{i}$'s for the sub-optimal arms are very small. Preceding a round based algorithm with an initial exploration of arms decreases the error probability. It is important to note that in theround based stage of our algorithm we do not wait till 1 arm is left, rather we use an early stopping condition and output 1 arm (even though it is not the last arm left) with an acceptable error probability. Additionally, we use a two arm deletion condition and prove that the initial exploration results in quicker deletion of arms. Another difference in our study is that we combine two different learning agents into one algorithm in a sequential manner(unlike Adaptive UCB-E or Successive Reject) and call them Phase1 and Phase2. We must point out that rather than exploring each arm equal number of times in Phase1 we go for UCB1. This results in a more comprehensive exploration of the arms initially and also reduces regret. Deciding the transition time from Phase1 to Phase2 is however a challenge. %First we came the idea of Hardness of a bandit problem as defined in \cite{audibert2010best} to define the duration of Phase1. But as stated in \cite{audibert2010best}, that this hardness is an intractable quantity and over or under-estimating leads to problems. 
%We define the duration of Phase1 through a parameter ($\epsilon$) as defined in \cite{bubeck2013bounded} such that the $\min{\Delta_{i}} \geq \epsilon$ and $\epsilon>0$.
%\paragraph{}Summarizing the contributions below:-
%\begin{enumerate}
%\item In this paper, we have studied the effect of stacking one bandit algorithm over another. The algorithm is divided into phases. Phase1 is used for exploration and Phase2 which is a round-based method is used to output the optimal arm. We have proposed that stacked learning agents perform better than individual agents. 
%%\item To enforce a stopping condition to move from Phase1 to Phase2 we have proposed that the learner must know atleast a lower bound over $\min_{i\in A}{\Delta}_{i}$.
%\item We have proved that the exploration of arms in Phase1 results in quicker deletion of arms in Phase2.
%\item In Phase2 two different arm deletion conditions has been employed resulting in lesser regret. We have made the deletion of arms a function of $|A|$ which results in faster deletion of arms. To our knowledge, our approach is the first one to combine two arm deletion conditions in a single round for round-wise algorithms. 
%\item We implemented early stopping condition which results in accumulation of lesser cumulative regret. This is also a new approach as most of the round -wise or phase wise algorithm wait till one arm is left as in \citep{auer2010ucb}, \citep{even2006action} or \citep{audibert2010best}.
%\item For a large number of arms this algorithm performs better compared to other round-based algorithms such as UCB-Revisited, Successive Elimination and Median Elimination. 
%\item To our knowledge this is the first attempt to make an improvement over UCB-Revisited, while following a similar approach but combining or augmenting the learning of another algorithm (UCB1) which precedes it.
%\end{enumerate} 

From a theoretical viewpoint, we conclude that the gap-dependent regret bound of ClusUCB is lower than that of MOSS and UCB-Improved. From the numerical experiments on settings with small gaps between optimal and sub-optimal mean rewards, we observed that ClusUCB outperforms several popular bandit algorithms. 
While we exhibited better regret bounds for ClusUCB, it would be interesting future research to improve the theoretical analysis of ClusUCB to achieve the gap-independent regret bound of MOSS and possibly also the gap-dependent bound conjectured in Section 2.4.3 of \cite{bubeck2012regret}.