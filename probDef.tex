%Formally, the domain is characterized by the following
%model:
%\begin{itemize}
%\item 
%\end{itemize}
%• A set of possible customer states, X. States can represent intrinsic
%features of the customer such as gender and age, but also dynamic
%quantities such as the items in his shopping cart, or his willingness
%to shop;
%• A set of possible product suggestions and advertisements, A;
%• A probability kernel, P, defined below.
%An episode in the online shop domain begins at time t = 0, when
%a customer with features x0 2 X enters the online shop. Then, a sequential
%interaction between the customer and the online shop begins,
%where at each step t = 0, 1, 2, . . . , an advertisement at 2 A is shown to the customer, and following that the customer makes a decision to
%either (i) add a product to his shopping cart; (ii) not buy the product,
%but continue to shop; (iii) stop shopping and check out. Following the
%customers decision, his state changes to xt+1 (reflecting the change in
%the shopping cart, willingness to continue shopping, etc.). We assume
%that this change is captured by a probability kernel P(xt+1| xt, at).
%When the customer decides to check out, the episode ends, and a
%profit is obtained according to the items he had added to his cart. The
%goal is to find a product suggestion policy, x ! a 2 A, that maximizes
%the expected total profit.
%When the probabilities of customer responses P are known in advance,
%calculating an optimal policy for the online shop domain is basically
%a planning problem, which may be solved using traditional methods
%for resource allocation [Powell, 2011]. A more challenging, but realistic,
%scenario is when P is not completely known beforehand, but has
%to be learned while interacting with customers. A second advantage concerns what is known as the exploitation–
%exploration dilemma: should the decision-maker display only the most
%profitable product suggestions according to his current knowledge
%about P, or rather take exploratory actions that may turn out to be
%less profitable, but provide useful information for future decisions? The
%Bayesian method offers a principled approach to dealing with this difficult
%problem by explicitly quantifying the value of exploration, made
%possible by maintaining a distribution over P. The various parameter configurations in the online shop domain
%lead to the different learning problems surveyed in this paper. In particular:
%• For a single-step interaction, i.e., when the episode terminates
%after a single product suggestion, the problem is captured by the
%multi-armed bandit model of Chapter 3.

We formally define the MAB model in this section. We will mainly focus on the stochastic MAB model in our work. In this setting, the learning algorithm is provided with a set of decisions (or arms) with reward distributions unknown to the algorithm. These decisions can be the items to show the customers, or medical treatment, or the stock options. The learning proceeds in an iterative fashion, where in each timestep, the algorithm chooses an arm and receives a stochastic reward that is drawn from a stationary distribution specific to the arm selected.  There are two types of goal we will be focusing our work on:-

\begin{enumerate}
\item Maximizing cumulative reward over the entire time horizon
\item Minimizing probability of error in suggesting the best arm(s) at any timestep
\end{enumerate}

Given the goal of maximizing the cumulative reward, the learning algorithm faces the exploration-exploitation dilemma, i.e., in each round should the algorithm select the arm which has the highest observed mean reward so far 
(\textit{exploitation}), or should the algorithm choose a new arm to gain more knowledge of the true mean reward of the arms and thereby avert a sub-optimal greedy decision (\textit{exploration}). 

Formally, let $r_i$, $i=1,\ldots,K$ denote the mean rewards of the $K$ arms and $r^* = \max_i r_i$ the optimal mean reward. The objective in the stochastic bandit problem is to maximize cumulative reward by  minimizing the cumulative regret, which is defined as follows:
\begin{align*}
R_{T}=r^{*}T - \sum_{i\in A} r_{i}N_{i}(T),
\end{align*}
where $T$ is the number of rounds, $N_{i}(T)=\sum_{m=1}^T I(I_m=i)$ is the number of times the algorithm chose arm $i$ up to round $T$.
The expected regret of an algorithm after $T$ rounds can be written as
%\newline
%\newline
\begin{align*}
\E[R_{T}]= \sum_{i=1}^K \E[N_i(T)] \Delta_i,
\end{align*}
where $\Delta_{i}=r^{*}-r_{i}$ denotes the gap between the means of the optimal arm and of the $i$-th arm. 

	In the pure exploration thresholding bandit setup the goal is different than minimizing the cumulative regret. Here the learning algorithm is provided with a threshold $\tau$ and it has to output all such arms $i$ whose mean of reward distribution $r_{i}$ is above $\tau$ after $T$ rounds. This is a specific instance of combinatorial pure exploration where the learning algorithm can explore as much as possible given a fixed horizon $T$ and not be concerned with the usual exploration-exploitation dilemma. Let $A$ be the set of all arms. Formally we can define a set $S_{\tau}=\lbrace i\in A: r_{i}\geq \tau \rbrace$ and the complementary set $S_{\tau}^{C}=\lbrace i\in A: r_{i} < \tau \rbrace$. Also we define $\hat{S}_{\tau}=\hat{S}_{\tau}(T)\subset A$ and its complementary set $\hat{S}_{\tau}^{C}$ as the recommendation of the learning algorithm after $T$ rounds. Given such sets exists, the performance of the learning agent is measured by how much accuracy it can discriminate between $S_{\tau}$ and $S_{\tau}^{C}$ after time horizon $T$.  The loss $\Ls$ is defined as:-
\begin{align*}
\Ls (T) = I\big(\lbrace S_{\tau}\cap \hat{S}_{\tau}^{C}\neq \emptyset\rbrace    \cup    \lbrace\hat{S}_{\tau}\cap S_{\tau}^{C}\neq \emptyset\rbrace\big)
\end{align*}	
		
The goal of the learning agent is to minimize $\Ls (T)$. So, the expected loss after $T$ rounds is 
\begin{align*}
\E[\Ls(T)] = \Pb\big(\lbrace S_{\tau}\cap \hat{S}_{\tau}^{C} \neq \emptyset \rbrace  \cup   \lbrace \hat{S}_{\tau}\cap S_{\tau}^{C} \neq \emptyset\rbrace\big)
\end{align*}

which we can say is the probability of making mistake, that is whether the learning agent at the end of round $T$ rejects arms from $S_{\tau}$ or accepts arms from $S_{\tau}^{C}$ in its final recommendation. Also, we are looking at an anytime algorithm, so the knowledge of $T$ may not be known to the learner.


%Another metric to judge the performance of an algorithm is the notion of simple regret whereby the learning algorithm tries to maximize instantaneous reward at the end of the time horizon $T$  and minimizing the simple regret. Simple regret is defined by:-
%\begin{align*}
%R'_{T}=r^{*} - r_{J_{T}}, \text{where $J_{T}$ is the recommended arm at the end of time horizon $T$}
%\end{align*}
%
%In the motivation section, example $1,2$ and $3$ are the problems minimizing cumulative regret, whereas $4$ and $5$ deal with minimizing simple regret. The latter problem in bandit literature is also known as \textit{pure exploration bandits}. 

%The problem, gets more difficult when the $\Delta_{i}$'s are smaller and arm set is larger. Also let $\Delta=min_{i\in A}\Delta_{i}$, that is it is the minimum possible gap over all arms in $A$.
                                                                                                                                         