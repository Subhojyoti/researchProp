%NEW RELATED WORK AND CONTRIBUTION

An early work involving a bandit setup is \cite{thompson1933likelihood}, where the author deals the problem of choosing between two treatments to administer on patients who come in sequentially. Following the seminal work of  \cite{robbins1952some}, bandit algorithms have been extensively studied in a variety of applications. 
From a theoretical standpoint, an asymptotic lower bound for the regret was established in \cite{lai1985asymptotically}. In particular, it was shown that for any consistent allocation strategy, we have
$\liminf_{T \to \infty}\frac{\E[R_{T}]}{\log T}\geq\sum_{\{i:r_{i}<r^{*}\}}\dfrac{(r^{*}-r_{i})}{D(p_{i}||p^{*})},$
where $D(p_{i}||p^{*})$ is the Kullback-Leibler divergence between the reward densities $p_{i}$ and $p^{*}$, corresponding to arms with mean $r_{i}$ and $r^{*}$, respectively.

	There have been several algorithms with strong regret guarantees. The foremost among them is UCB1 by  \cite{auer2002finite}, which has a regret upper bound of $O\bigg(\dfrac{K\log T}{\Delta}\bigg)$, where $\Delta = \min_{i:\Delta_i>0} \Delta_i$. This result is asymptotically order-optimal for the class of distributions considered. However, the worst case gap independent regret bound of UCB1  can be as bad as $O \bigg(\sqrt{TK\log T}\bigg)$.  In \cite{audibert2009minimax}, the authors propose the MOSS algorithm and establish that the worst case regret of MOSS is $O\bigg(\sqrt{TK}\bigg)$ which improves upon UCB1 by a factor of order $\sqrt{\log T}$. However, the gap-dependent regret of MOSS is  $O\left(\dfrac{K^{2}\log\left(T\Delta^{2}/K\right)}{\Delta}\right)$ and in certain regimes, this can be worse than even UCB1 (see \cite{audibert2009minimax},\cite{lattimore2015optimally}). The UCB-Improved algorithm, proposed in \cite{auer2010ucb}, is a round-based algorithm\footnote{An algorithm is \textit{round-based} if it pulls all the arms equal number of times in each round and then proceeds to eliminate one or more arms that it identifies to be sub-optimal.} variant of UCB1 that 
has a gap-dependent regret bound of $O\bigg(\dfrac{K\log T\Delta^{2}}{\Delta}\bigg)$, which is better than that of UCB1. On the other hand, the worst case regret of UCB-Improved is $O\bigg(\sqrt{TK\log K}\bigg)$. 

	In the pure exploration setup, a significant amount of research has been done on finding the best arm(s) from a set of arms. The pure exploration setup has been explored in mainly two settings:-
\begin{enumerate}
\item Fixed Budget setting: In this setting the learning algorithm has to suggest the best arm(s) within a fixed number of attempts that is given as an input. The objective here is to maximize the probability of returning the best arm(s). One of the foremost papers to deal with single best arm identification is \cite{audibert2009exploration} where the authors come up with the algorithm UCBE and Successive Reject(SR) with simple regret guarantees. The relationship between cumulative regret and simple regret is proved in \cite{bubeck2011pure} where the authors prove that minimizing the simple regret necessarily results in maximizing the cumulative regret. In the combinatorial fixed budget setup \cite{gabillon2011multi} come up with Gap-E and Gap-EV algorithm which suggests the best $m$ (given as input) arms at the end of the budget with high probability. Similarly, \cite{bubeck2013multiple} comes up with the algorithm Successive Accept Reject(SAR) which is an extension of the SR algorithm. SAR is a round based algorithm whereby at the end of round an arm is either accepted or rejected based on certain conditions till the required top $m$ arms are suggested at the end of the budget with high probability. 
\item Fixed Confidence setting: In this setting the the learning algorithm has to suggest the best arm with a fixed (given as input) confidence with as less number of attempts as possible. The single best arm identification has been handled in \cite{even2006action} where they come up with an algorithm called Successive Elimination which comes up with an arm that is $\epsilon$ close to the optimal arm. In the combinatorial setup recently \cite{kalyanakrishnan2012pac} have suggested the LUCB algorithm which on termination returns $m$ arms which are atleast $\epsilon$ close to the true top $m$ arms with $1-\delta$ probability.
\end{enumerate}	

	Apart from these two settings some unified approach has also been suggested in \cite{gabillon2012best} which proposes the algorithms UGapEb and UGapEc which can work in both the above two settings. A similar combinatorial setup was also explored in \cite{chen2014combinatorial} with one major difference. Instead of suggesting the top-$m$ arms the learning algorithm, called Combinatorial Successive Accept Reject (CSAR) is provided with a threshold $\tau$ and it has to suggest all the arms such that $r_{i}\geq \tau, \forall i\in A$. In the latest work in \cite{locatelli2016optimal} the algorithm Anytime Parameter-Free Thresholding (APT) algorithm comes up with a better anytime guarantee than CSAR.  


%\todos{What regime is this? Can you give a reference here?} 
	
	%But this algorithm performs worse empirically as stated in \cite{lattimore2015optimally}. \textit{In this work we try to address one of the open questions as raised in \cite{bubeck2012regret} that is to find an algorithm with regret always better than MOSS and UCB-Improved.}
	%
	%In this context of Multi-Armed Stochastic Bandit, we will also like to mention some of the variance based algorithm such as UCB-Normal(\cite{auer2002finite}) , UCB-Tuned(\cite{auer2002finite}) and UCB-Variance(\cite{audibert2009exploration}) which shows that variance-aware algorithms tend to perform better than the algorithms that don't(UCB1, UCB-Improved, MOSS). It can be shown that  when the variance of some sub-optimal arm is lower, a variance-aware algorithm detects it quickly thereby reducing  regret. \textit{We test our algorithm against such variance-aware techniques and do an empirical analysis.} 
	%
	%A discussion on some recent algorithms employing divergence based techniques must also be done. Firstly, the algorithm proposed in \cite{honda2010asymptotically} the authors come up with the algorithm Deterministic Minimum Empirical Divergence also called DMED$+$(as referred by \cite{garivier2011kl}) which is first order optimal(they only give asymptotic guarantees). This algorithm keeps a track of arms whose empirical mean are close to the optimal arm and takes help of large deviation ideas to find the optimal arm. Secondly, the more recent algorithm called KL-UCB(\cite{garivier2011kl}) using KL-Divergence comes up with an upper bound on regret as $\sum_{i:\Delta_{i}>0}\bigg(\dfrac{\Delta_{i}(1+\alpha)\log T}{D(r_{i},r^{*})}+C_{1}\log\log T+\dfrac{C_{2}(\alpha)}{T^{\beta(\alpha)}}\bigg)$ which is strictly better than UCB1 as we know from Pinsker's inequality $D(r_{i},r^{*}) > 2\Delta_{i}^{2}$. KL-UCB beats UCB1, MOSS and UCB-Tuned in various scenarios. \textit{We empirically test against this algorithm 
% and show that in certain regimes our algorithm performs better than KL-UCB and DMED.}
	%
	%We also make a distinction between round-based algorithms like UCB-Improved, Successive Reject(\cite{audibert2010best}), Successive Elimination(\cite{even2006action}) and Median-Elimination(\cite{even2006action}) whereby the algorithm pulls all the arms equal number of times in each round/phase, then proceeds to eliminate some number of arms it identifies to be sub-optimal with high probability and this continues till you are left with one arm. \textit{Our algorithm is also a round based algorithm}. Finally, we must point out that our setup is strictly limited to stochastic scenario as opposed to adversarial setup as studied in the paper \cite{auer2002nonstochastic} where an adversary sets the reward for the arms for every timestep.


%On four synthetic setups with small gaps, we observe empirically that ClusUCB outperforms UCB-Improved\cite{auer2010ucb} and MOSS\cite{audibert2009minimax} as well as other popular stochastic bandit algorithms such as DMED\cite{honda2010asymptotically}, UCB-V\cite{audibert2009exploration}, Median Elimination\cite{even2006action}, Thompson Sampling\cite{agrawal2011analysis} and KL-UCB\cite{garivier2011kl}.

% 	Summarizing our contributions below:-
% \begin{enumerate}
% \item We propose a cluster based round-wise algorithm with two arm elimination conditions in each round.
% \item We achieved a lower regret upper bound(Theorem1,table in Appendix F) than UCB-Improved(\cite{auer2010ucb}), UCB1(\cite{auer2002finite}) and  MOSS (\cite{audibert2009minimax} which we verify both theoretically and empirically.
% \item Our algorithm also empirically compares well with DMED, UCB-Varinace and KL-UCB.
% \item In the critical case when $r_{1}=r_{2}=..=r_{K-1}<r^{*}$ and $\Delta_{i}$'s are small and $K$ is large which is encountered frequently in web-advertising domain (as stated in \cite{garivier2011kl}) this approach has a significant advantage over other methods.
% \item The only parameter that needs to be pre-specified to the algorithm is the number of clusters to be formed but unlike KL-UCB our algorithm parameter $p$ is not distribution-specific and also our algorithm does not involve calculation of a complex, time consuming function like the divergence function of KL-UCB.
% %though the authors specified in \cite{garivier2011kl} that for optimal result only one should use the divergence function specific to the type of distribution.
% %\item We also provide a short discussion on what other applications our algorithm can be employed successfully.
% \end{enumerate}
	
%The rest of the paper is organized as follows: In Section \ref{sec:clusucb}, we present the ClusUCB algorithm. In Section \ref{sec:results}, we present the associated regret bounds and prove the main theorem on the regret upper bound  for ClusUCB in Section \ref{sec:proofTheorem}. In Section \ref{sec:expts}, we present the numerical experiments and provide concluding remarks in Section \ref{sec:conclusions}. Further proofs of corollaries and propositions presented in Section \ref{sec:proofTheorem} are provided in the appendices. 

%Appendix \ref{App:A}, \ref{App:B} deals with proofs of  2  propositions which are derived from our main regret bound theorem. Appendix \ref{App:Proof:Corollary:1}, \ref{App:Proof:Corollary:2}, \ref{App:Proof:Corollary:3} deals with proofs of 3 corollaries which specializes the result of our main regret bound theorem. Appendix \ref{App:D} deals with exploration regulatory factor and appendix \ref{App:E} explains why we do clustering. The last appendix \ref{App:Further:Expt} deals with further experiments on two different testbeds.
